{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiber Photometry Demo\n",
    "\n",
    "This notebook contains exercises split across multiple lessons that span the following primary topics\n",
    "* strings\n",
    "* conditionals\n",
    "* lists\n",
    "* pandas\n",
    "* dictionaries\n",
    "* visualization\n",
    "\n",
    "The exercise modules are centered around analysis of neuroscience fiber photometry data, which involves recording __bulk__ changes in brightness of activity-dependent fluorescent proteins that are expressed in cells of interest. Typically the activity data stream consists of a single vector time-series (i.e. fluorescence intensity measurements acquired across time). For this particular dataset (acquired by Dr. Adam Gordon-Fennell), animals were periodically allowed to consume liquid rewards in \"access periods.\" Both the onsets of the access periods and animal licks were recorded. The data consist of the following files:\n",
    "\n",
    "* *_events.csv : table where rows contain the event type (access period or lick) and time of occurrence (in seconds)\n",
    "* *_streams_session.csv : table where rows are samples acquired across time for the photometry recording. For each sample, the table details the channel, time, and fluorescence value. __The data contain activity traces for a 405nm isosbestic control channel and a 465nm GCaMP signal.__\n",
    "\n",
    "The exercises follow a sequence starting from loading in (behavioral and neural stream) data from tabular files (excel CSVs), examining the data and extracting relevant data streams, preprocessing the data, and visualizing the activity. In the 2nd half of the exercises, we incorporate the behavioral data and generate activity plots centered around the behavioral events. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro exercise: Importing packages\n",
    "\n",
    "Usually the very beginning of a script (besides commentary and markdown) consists of package imports. Below I've already specified a few packages to import. We still need to import the `pandas` package. Go ahead and add pandas to the import list and abbreviate it to `pd`\n",
    "\n",
    "If the import is successful, the following lines should execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(suppress=True) # turns scientific notation off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I set the directory and file name that we want to load\n",
    "fdir = r'sample_data/' \n",
    "fname = '2022_06_10_abb12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_feather(fdir + fname + '_events.feather') # just a temporary view of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditionals exercise 1: Detecting and loading data in different file formats.\n",
    "\n",
    "The cell below defines the absolute file path (`epoc_data_path`) for the behavioral data. Currently we are loading in the .feather file, which is just another file format that can store tabular data, as opposed to csv or xlsx formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note os.path.join is a function that simply merges and formats string arguments together such that they are compatible with other file loading functions\n",
    "epoc_data_path = os.path.join(fdir, f'{fname}_events.feather') \n",
    "epoc_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's start with a practice example:\n",
    "\n",
    "Notice that we are using the pandas package (abbreviated in the import code section at the top of this notebook as `pd`) to load the tabular data. We will learn more about it later, but for now, pandas is a package that adds tabular data processing capabilities to python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'feather' in epoc_data_path:\n",
    "    print('loading feather file')\n",
    "    epoc_data = pd.read_feather(epoc_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Complete the following_ (Conditionals exercise 1)\n",
    "\n",
    "I've provided the contents of the if statements, please replace the lines marked with my comments with an `if` statements checking for `'feather'` files, an `elif` checking for `'csv'` files, and a catch-all `else` statement for the variable `epoc_data_path`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'feather' in epoc_data_path:\n",
    "    print('loading feather file')\n",
    "    epoc_data = pd.read_feather(epoc_data_path)\n",
    "elif 'csv' in epoc_data_path:\n",
    "    print('loading csv file')\n",
    "    epoc_data = pd.read_csv(epoc_data_path)\n",
    "else:\n",
    "    print('Not a valid file type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sample_data folder, we also have a csv version - try changing the behavioral data path above (`epoc_data_path`) to the csv version, then rerun the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas and methods preview! We'll use these to view the data that we loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the .head() method on our pandas dataframe will show the top 5 entries.\n",
    "epoc_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditionals exercise 2: Checking contents of lists using conditionals and boolean operators\n",
    "### Also list preview! Lists (name is kind of self-explanatory) are a type of data format that stores a sequence of items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preview of the behavioral data table above exercise doesn't really tell us how many behavioral conditions there are (event_id_char column) since there are so many row entries. We can have python look through all entries in the event_id_char column and tell us what unique entries there are by using the .unique() method. We will go over what coding concept a method is in later lessons, but for now, just know it will return all unique entries for a given pandas dataframe/series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_conditions = list(epoc_data['event_id_char'].unique()) # grab unique condition names\n",
    "event_conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Complete the following_ (Conditionals exercise 2)\n",
    "\n",
    "Now that we have a list that contains all unique conditions present in the data, we might want to automatically check if particular conditions are present. To do that we could leverage if statements and boolean operators. Below, please fill in the underscored parts to enable \n",
    "1) the first portion of the if statement to check if BOTH 'access_period' AND 'lick' are in the conditions list (`event_conditions`).\n",
    "2) the second portion of the statement (`elif`) to check for at least one of the above conditions are present\n",
    "3) bonus is to add a catch-all in case neither condition is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'access_period' in event_conditions and 'lick' in event_conditions:\n",
    "    print('Both conditions present')\n",
    "elif 'access_period' in event_conditions or 'lick' in event_conditions:\n",
    "    print('One out of the two conditions present')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas exercise 1: Exploring contents of a dataframe\n",
    "\n",
    "Use and familiarize yourself with various pandas dataframe methods to get a sense of the contents in the behaviora epoch dataframe (`epoc_data`).\n",
    "\n",
    "### _Complete the following_ (Pandas exercise 1)\n",
    "\n",
    "__Please use the nunique, info, head, tail, shape methods (in separate cells). Read the documentation to understand what info is output and what arguments you can input into the method to customize the information delivered. In another cell, please also use dataframe slicing to identify which row do the event conditions switch (from access_period to lick, for example - you will have to determine this empirically or check the csv file!).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoc_data.nunique() # provides count of unique entries in each column\n",
    "epoc_data.info() # can identify if there is missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoc_data.head() # check the first 5 entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoc_data.tail() # check the last 5 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoc_data.iloc[95:105] # use dataframe slicing/indexing to find which line the behavioral conditions change from access_period to lick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoc_data.shape # check the dimensions of the dataframe\n",
    "# we can also get a sense of how many events (y dimension) there are. By using this method, the first dimension outputted is y and the 2nd in x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas exercise 2: Obtaining data from a dataframe column based on information in another column.\n",
    "\n",
    "We are working with neural activity data that was synchronized to events that happened during the recording. Our behavioral dataframe contains event times (event_ts column) for two different types of behavioral events (`event_id_char` column: access_periods, licks, and high-fives). Here let's figure out how to extract event times for a specific condition, `access_period`.\n",
    "\n",
    "### _Complete the following_ (Pandas exercise 2.1)\n",
    "\n",
    "First we need to identify which rows in `epoc_data` correspond to the `access_period` condition. __Please use the `==` comparison to make this comparison and assign this boolean list/series to the variable below__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_access_period = epoc_data['event_id_char'] == 'access_period'\n",
    "boolean_access_period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use our boolean mask that indicates which rows are `access_period` trials to extract the corresponding time stamps of those events. \n",
    "\n",
    "### _Complete the following_ (Pandas exercise 2.2)\n",
    "\n",
    "__Please fill in the line below with dataframe conditional selection to extract timestamps from the `event_ts` column and `access_period` trials indexed using the `boolean_access_period` variable.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab event times for condition 1\n",
    "event_one = epoc_data['event_ts'][boolean_access_period].values\n",
    "event_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we'll begin to work with the time-series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas, math, and functions combined exercise: Correcting photometry traces with isosbestic channel\n",
    "\n",
    "Our goal here is to ultimately plot the photometry activity trace across the whole session in one window. Specifically we want to plot the 405 isosbestic, 465 GCaMP, and the 405-corrected signal in the same plot. \n",
    "\n",
    "In the cell immediately below, we'll read in the raw photometry activity table data. It has columns for timestamps, raw activity values, and channel (405nm vs 465nm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(fdir, f'{fname}_streams_session.feather')\n",
    "\n",
    "data = pd.read_feather(data_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract the raw activity signals for the 405nm and 465nm channels separately (they are currently stacked on top of eachother in the dataframe). \n",
    "\n",
    "### _Complete the following_ (Pandas exercise 3.1)\n",
    "\n",
    "__Please use conditional selection again (similar principle as the previous exercise) to first identify rows that correspond to 405nm and then use that boolean mask to extract activity data (`signal`) from the `data` dataframe.__ \n",
    "\n",
    "*** hint: the 405 and 465 entries are strings, not actual numbers, so you will need to encompass the numbers with quotes like `'405'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I've kept the .values method because it converts the data \n",
    "from a pandas series to a numpy array, which in my opinion \n",
    "and experience is easier to work with.\n",
    "\"\"\"\n",
    "data_405 = data['signal'][data['channel'] == '405'].values\n",
    "data_465 = data['signal'][data['channel'] == '465'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's obtain the time-stamps across all the datapoints. Remember this information is also in the `data` variable (column is called `'time'`). In this particular \"tidy\" format of the `data` sheet, we have 405 channel data stacked on top of the 465 data. Because of that, each channel possesses a duplicate of the time-stamps across the whole session (you can see in the cell below that when the channel transitions to 465nm, the time stamps reset). So we only need the time-stamps from one channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[31920:31928, 'time':'channel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Below, please extract the `'time'` values from the `data` dataframe by filling in the underscored section below, and assigning to the variable `tvec` (time vector, abbreviated).__\n",
    "\n",
    "### _Complete the following_ (Pandas exercise 3.2)\n",
    "\n",
    "`tvec_zeroed` is simply tvec but shifted such that the first timepoint is 0. Note we are only using this for plotting purposes for this particular exercise section. We do not need to normalize the tvec for subsequent exercises!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = data['time'][data['channel'] == '405'].values \n",
    "tvec_zeroed = tvec - data['time'][0] # for plotting trace starting from time 0\n",
    "\n",
    "print(tvec_zeroed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular method for correcting the GCaMP signal using the isosbestic signal is to first calculate a linear least squares (LLS) fit between the GCaMP and isosbestic signal, apply the transform to the isosbestic signal (fitted control), and subtract the fitted control trace from the GCaMP signal\n",
    "\n",
    "I've precoded a couple of functions that will help with this process. \n",
    "\n",
    "The first function will perform the fit between the 405 and 465 channels, and apply the transform to the 405 isosbestic channel. This generates a fitted control as the output.\n",
    "\n",
    "What's going on mathematically here? The `np.polyfit(x,y,1)` function is essentially a linear regression `y=mx+b` equation. Let's think of it in this context.\n",
    "\n",
    "<img src=\"pics\\LLS.jpg\" width=\"420\" />\n",
    "\n",
    "The second function will apply a delta F/F normalization calculation. This consists of subtracting the fitted control signal from the 465 GCaMP signal sample-by-sample. This yields the fully-corrected signal that can be used for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controlFit(control, signal):\n",
    "    \n",
    "    # GuPPY version\n",
    "    # function to fit control channel to signal channel\n",
    "\n",
    "    p = np.polyfit(control, signal, 1) # p are the polynomial coefficients for the line that best fits control (x) to signal (y)\n",
    "    arr = (p[0]*control)+p[1] # apply transform coefficients to control channel to generate fitted control\n",
    "    return arr\n",
    "\n",
    "\n",
    "def deltaFF(signal, control):\n",
    "    \n",
    "    # function to compute deltaF/F using fitted control channel and filtered signal channel\n",
    "\n",
    "    res = np.subtract(signal, control) # numerator of F(t)-f0\n",
    "    normData = np.divide(res, control) # (F(t)-f0)/F0\n",
    "    normData = normData*100\n",
    "\n",
    "    return normData\n",
    "\n",
    "fitted_control = controlFit(data_405, data_465) # this function would be helpful if we wanted to correct signals from multiple fluorophore channels\n",
    "corrected_data = deltaFF(data_465, fitted_control) # this function would be helpful everytime we needed to compute dF/F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the final plot! No need to do anything besides execute the cell below and enjoy the fruits of your labor!\n",
    "\n",
    "Notice how this normalization method corrects the photobleaching/slow drift trend and also sets the baseline close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(tvec_zeroed, data_405)\n",
    "plt.plot(tvec_zeroed, data_465)\n",
    "plt.plot(tvec_zeroed, fitted_control)\n",
    "plt.plot(tvec_zeroed, corrected_data)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Fluorescence (dF/F)')\n",
    "#plt.xlim([170, 400]) # we can zoom in to see how motion artifacts can be corrected for\n",
    "plt.legend(['405', '470', 'fit curve', 'lls-corrected']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary Exercise 1: Dividing event times up into their respective conditions. These event times can then be organized into a dictionary for storage purposes.\n",
    "\n",
    "Now that we understand how the behavioral and time-series data are formatted, let's start to think about how we can link the two data formats. First we know there are multiple event types (e.g. ccess_period and licks) in the behavior and that for each event we have a corresponding time point that it happened at. When we ultimately view the event-related data, we probably want it organized by event type. So the next step is to group the event times based on event type, which can be done using.\n",
    "\n",
    "### _Complete the following_ (Dictionary exercise 1)\n",
    "\n",
    "__Please fill in the cell below to extract time-stamps for each event and organize them by conditions into a dictionary.__\n",
    "\n",
    "** hint the lines within the for loop that extracts the event times can be formatted similar to how we extracted time-stamps for the 'access_period' events back in the pandas lesson. The variable name was `event_one` and we used conditional selection. Think about how to interface a for loop with conditional selection. \n",
    "\n",
    "Also, just a reminder to use the .values method on the selected series to extract data as a numpy array, if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab event times for all conditions via for loop\n",
    "event_times_dict = {}\n",
    "for event_name in epoc_data['event_id_char'].unique():\n",
    "    \n",
    "    boolean_event_type = epoc_data['event_id_char'] == event_name\n",
    "    event_times_dict[event_name] = epoc_data['event_ts'][boolean_event_type].values\n",
    "\n",
    "event_times_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just reading: Convert behavioral event times to samples: to prepare for indexing and snipping out trial activity windows.\n",
    "\n",
    "#### Requires knowledge on: dictionaries and functions \n",
    "\n",
    "The whole-session activity trace, while nice to see, gives us very limited understanding of what is happening when the animal performs a behavior or reacts to a manipulation. \n",
    "* To get a better sense, we will need to get closeup snapshots of activity centered around events. \n",
    "* In other words, we need to identify when in the activity data an event occurred and then extract the surrounding datapoints.\n",
    "\n",
    "The activity data itself, in isolation, is not characterized by units of time, rather each item in the vector is a \"sample\" in the recording and does not hold any information on timing. We do however have the time vector column (we assigned this to the `tvec` variable) that indicates for each frame what time they were acquired at, which will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand our behavioral events are in units of time, and to make things slightly complicated, they may not be occurring exactly when a photometry data point was acquired. Also in our case, there are missing timestamps for our photometry data, so directly converting behav time stamps to photometry data samples does not work.\n",
    "\n",
    "So we need to identify the photometry datapoint that occurred nearest to when the behavioral event happened. To do this, we use the time vector `tvec` and each behavioral event to match the closest datapoint to an event occurrence\n",
    "\n",
    "We can use this line of code : `np.argmin(abs(tvec - event_time))`\n",
    "\n",
    "Here by subtracting a single event time from tvec, the closest datapoint timestamp to the event time will be 0 or near-zero. This sets things up nicely for us to use `np.argmin()` (we learn about this in the numpy lesson) to identify the index of the minimum value in the vector. `abs` is used to flip the sign of negative numbers so that they don't count in the minimum value search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'First event from condition one (event time): {event_one[0]} seconds')\n",
    "\n",
    "print(f\"The photometry sample that corresponds to {event_one[0]} sec is {np.argmin(abs(tvec - event_one[0]))}\")\n",
    "\n",
    "print(f\"Photom sample {np.argmin(abs(tvec - event_one[0]))}'s time: {tvec[4224]} seconds\") # if dataset changes, replace number in tvec[4224] with calculated sample number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the slight difference in the actual event time compared to the calculated photometry sample time because the two data aren't synchronously acquired (at least in this dataset).\n",
    "\n",
    "In the cell below, I've turned the above time-stamp to samples (using the `tvec` variable) conversion into a repeatable function for our next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the above code into a function\n",
    "def get_tvec_sample(tvec, time):\n",
    "    return np.argmin(abs(tvec - time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary exercise 2: To analyze all trials, let's do some organization of the event conditions and trial times into a dictionary\n",
    "\n",
    "In this exercise let's create a dictionary similar to the one in exercise 1, but now all the contents will be in samples instead of time. This will make it easier for us to extract data from the whole session activity trace using the appropriate indexing. \n",
    "\n",
    "### _Complete the following_ (Dictionary exercise 2)\n",
    "\n",
    "__Although there are different ways to do this, the method that I would like you to practice is using a for loop to go through the event types. Within this for loop, we will use a second for loop to go through the events in `event_times_dict` and convert each time-stamp to samples using the function I provided above. IMPORTANTLY, this method requires the use of initializing an empty list that will be populated with the converted event samples and finally assigned to a dictionary key. Please fill in the lines below.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_samples_dict = {} # initialize your dictionary\n",
    "for condition_name in epoc_data['event_id_char'].unique(): # loop through conditions\n",
    "    \n",
    "    tmp_list = [] # initialize your list\n",
    "    \n",
    "    # go through each event and compute the sample/frame that it occurred on (b/c the list is currently in seconds)\n",
    "    for event in event_times_dict[condition_name]: # loop through events\n",
    "        tmp_list.append(get_tvec_sample(tvec, event))\n",
    "        \n",
    "    # once we go through converting each time to sample, and add to a list iteratively,\n",
    "    # store that list to its corresponding condition in the dictionary\n",
    "    event_samples_dict[condition_name] = tmp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two cells below allow us to see if the exercise code succeeded: 1) keys should show us all the event conditions names, 2) calling the value contents of a particular key of the dictionary should show us a list of event samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_samples_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_samples_dict['access_period']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy (and dictionary) exercise 1: Let's analyze data from one trial\n",
    "\n",
    "To extract trial-related activity data from the whole-session recording, we need the following information: \n",
    "* the event onset in units of samples (since the activity vector inherently does not hold information about time)\n",
    "* start and end times for the window\n",
    "* the sampling rate to convert start/end times to relative sample indices\n",
    "\n",
    "### _Complete the following_ (Numpy exercise 1.1)\n",
    "\n",
    "__Calculate the sampling rate using the time-vector that was calculated in the pandas exercise (`tvec`, which indicates the timestamp, in seconds, of each sample):__\n",
    "1. calculate the difference between subsequent value pairs of `tvec`\n",
    "2. take the mean across all entries of step 1\n",
    "3. divide 1.0 by the result from step 2, and assign to the variable `fs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 1.0/np.mean(np.diff(tvec))\n",
    "print(\"sampling rate:\", fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define the start and end times for our template event window (`event_window_time`). Then multiply those times by the sampling rate to obtain the start/end sample indices `event_window_samples`.\n",
    "\n",
    "### _Complete the following_ (Numpy exercise 1.2)\n",
    "\n",
    "Define a 1D vector for the variable `event_window_time` that contains 2 elements: 1) the event window start time and 2) the event window end time. I recommend using -1 and 10 so that things line up with the markdown text below; however you can adjust to your liking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize start/end times for trial window\n",
    "event_window_time = np.array([-1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_window_samples = (event_window_time*fs).astype(int) # samples are integers\n",
    "event_window_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the indices above, -20 corresponds to -1 seconds multiplied by the sampling rate (`fs`=20, which we calculated earlier in this notebook) and 199 corresponds to 10 seconds * `fs` (side note: it's not 200 b/c we used `np.arange`, which cuts off the last number).\n",
    "\n",
    "With the information above, we need to compute a vector `trial_template_indices` that contains the sample indices relative to event onset (sample 0) in preparation for extracting event-related activity in a trial window. Specifically we wanted to extract a window of time from -1 to 10 seconds relative to event onset (encoded in the variable `event_window_time` - see cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate vector of every sample between start and end samples\n",
    "# then offset to specific event's window by adding the event sample\n",
    "trial_template_indices = np.arange(event_window_samples[0], event_window_samples[1])\n",
    "trial_template_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the event sample to every entry of the `trial_template_indices` gives us the samples required to extract activity for ___this specific trial___ in the -1 to 10 second time window around event onset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we demonstrated how to convert time to samples in the previous lesson's exercise above  \n",
    "first_event_sample = np.argmin(abs(tvec - event_one[0]))\n",
    "first_event_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_samples = trial_template_indices + first_event_sample\n",
    "trial_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we generate a vector that maps time onto samples in the event trial. This is for plotting visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_window_num_samples = event_window_samples[1] - event_window_samples[0]\n",
    "tvec_trial = np.linspace(event_window_time[0], event_window_time[1], trial_window_num_samples)\n",
    "tvec_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Complete the following_ (Numpy exercise 1.3)\n",
    "\n",
    "Given our vector `trial_samples` that represents the indices in our recording that corresponds to the specific trial we want to visualize, use those indices to slice into the whole session's activity trace (`corrected_data`) in preparation for plotting. Assign it to the variable `corrected_data_trial`.\n",
    "\n",
    "It's very simple! You only need the variables outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_data_trial = corrected_data[trial_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tvec_trial, corrected_data_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CZ ADDED 4/3/2025\n",
    "\n",
    "#trial 1\n",
    "event_sample = np.argmin(abs(tvec - event_one[0])) # reminder: this fill find the frame that the event occurs during\n",
    "indices_to_extract = trial_template_indices + event_sample\n",
    "plt.plot(corrected_data[indices_to_extract])\n",
    "\n",
    "#trial 2\n",
    "event_sample = np.argmin(abs(tvec - event_one[1])) \n",
    "indices_to_extract = trial_template_indices + event_sample\n",
    "plt.plot(corrected_data[indices_to_extract])\n",
    "\n",
    "#trial 3\n",
    "event_sample = np.argmin(abs(tvec - event_one[2])) \n",
    "indices_to_extract = trial_template_indices + event_sample\n",
    "plt.plot(corrected_data[indices_to_extract])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy (and dictionary) exercise 2: Let's analyze data from ALL trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract data from each trial across the session, we will create a loop that iterates through each event and adds the event onset sample to each entry in the `trial_template_indices` vector, like we did in the single trial activity extraction in the dictionary exercises. Fortunately, in those exercises, we generated a dictionary  (`event_samples_dict`) that contains the onset samples for each event occurrence, making it easier for us to loop through all events to calculate the samples to extract.\n",
    "\n",
    "To be able to save the trial snippits for each conditions, our code block will be a bit more sophisticated than we are used to and employ a couple of nested for loops. The first one will loop through trial conditions broadly and the block within will go through each trial for a given condition and generate an array containing trial snippits. This array will be assigned to a dictionary key.\n",
    "\n",
    "### _Complete the following_ (Numpy exercise 2.1)\n",
    "\n",
    "Do achieve the above, use the partially filled out code below and complete the following:\n",
    "\n",
    "1. Initialize an array that can be populated with activity trace snippits from each trial (called `tmp_trial_array`)\n",
    "2. Within the event for loop, assign activity trace snippits for each trial to the corresponding row in `tmp_trial_array`\n",
    "3. Assign the completed `tmp_trial_array` to a `data_trial_dict` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trial_dict = {}\n",
    "for condition_name in epoc_data['event_id_char'].unique(): # loop through conditions\n",
    "    \n",
    "    # initialize a numpy array to populate with trial-extracted data\n",
    "    tmp_trial_array = np.ones([len(event_samples_dict[condition_name]), trial_window_num_samples])*np.nan\n",
    "    \n",
    "    # use trial index template, offset with event sample, and extract trial data\n",
    "    for idx, event_sample in enumerate(event_samples_dict[condition_name]): # loop through events\n",
    "        tmp_trial_array[idx,:] = corrected_data[trial_template_indices + event_sample]\n",
    "    \n",
    "    # once data are fully populated, add to dictionary\n",
    "    data_trial_dict[condition_name] = tmp_trial_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trial_dict['access_period'].shape\n",
    "\n",
    "# we expect the dimensions to be 100 trials by 220 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Complete the following_ (Numpy exercise 2.2)\n",
    "\n",
    "The final quick exercise is to take the array above that represents activity snippits from all \"access_period\" trials (`data_trial_dict['access_period']`) and take the mean across trials. Assign this output to the variable `mean_access_period` and you will get a sneakpeak of the visualization lesson/exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_access_period = np.mean(data_trial_dict['access_period'], axis=0)\n",
    "\n",
    "plt.plot(mean_access_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we suspect there is a trial that contains an artifact in a certain time window? How do we efficiently exclude it from the analysis? Let's use NaN (not a number) entries to replace unwanted segments of values. \n",
    "\n",
    "### _Complete the following_ (Numpy exercise 2.3) - Excluding numpy numerical data\n",
    "\n",
    "For this let's focus on a single trial. I've predefined the window of time in our first `access_peiod` event epoch to exclude (`exclude_times`). Using the start and end samples (`exclude_start_samp`, `exclude_end_samp`), slice into `corrected_data` and replace the appropriate window with NaNs. Then plot that trial's activity trace.\n",
    "\n",
    "* __Break glass in case of emergency: if you mess up and `corrected_data` gets drastically altered, you will have to re-execute the code below. This generates the `corrected_data` variable - which was originally calculated in the pandas exercise 3.2__\n",
    "\n",
    "`corrected_data = deltaFF(data_465, fitted_control)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_data = deltaFF(data_465, fitted_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_times = [0, 5] # time window to exclude/convert to nans\n",
    "\n",
    "exclude_start_samp = (event_samples_dict['access_period'][0] + exclude_times[0]*fs).astype(int)\n",
    "exclude_end_samp = (event_samples_dict['access_period'][0] + exclude_times[1]*fs).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_data[exclude_start_samp:exclude_end_samp] = np.nan * np.zeros( exclude_end_samp-exclude_start_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corrected_data[trial_samples[0]:trial_samples[-1]])\n",
    "plt.plot(corrected_data[trial_samples[0]:trial_samples[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Complete the following_ (Visualization exercise 1): Lineplots\n",
    "\n",
    "Given the timestamps vector for a trial window (`tvec_trial`) and the dictionary above that contains trial snippits of fluorescence activity for each behavioral condition (`data_trial_dict`), generate a single line plot that shows two traces/lines: the mean (across trials) for both behavioral conditions.\n",
    "\n",
    "Please also add axis labels and a legend.\n",
    "\n",
    "For bonus points, can you identify which lines of code contain redundant segments and thus would be ideal for turning into a for loop? If so, try to implement such a for loop.\n",
    "\n",
    "Hint: use the plot function in matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_names = list(data_trial_dict.keys())\n",
    "\n",
    "plt.plot(tvec_trial, np.mean(data_trial_dict[cond_names[0]], axis=0))\n",
    "plt.plot(tvec_trial, np.mean(data_trial_dict[cond_names[1]], axis=0))\n",
    "plt.ylabel('Fluorescence', fontsize=14)\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.legend(cond_names, fontsize=14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Complete the following_ (Visualization exercise 2): Trial-resolved heatmaps\n",
    "\n",
    "Now let's focus on one condition's activity for ALL trials - it may be useful to examine how heterogeneous or homogeneous activity is across trials. Here, pull data out from one condition and generate a heatmap. Please include axis labels and a colorbar.\n",
    "\n",
    "If you are feeling spicy, try creating a figure that contains two subplots so you can visualize both condition's trial-resolved activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data_trial_dict[cond_names[0]], aspect='auto')\n",
    "plt.ylabel('Trial', fontsize=14)\n",
    "plt.xlabel('Sample', fontsize=14)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Fluorescence', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4b42ec29e0d042ab71033bfd4ca780b7a8291c7b26e4c3b3923625b510f7afa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
